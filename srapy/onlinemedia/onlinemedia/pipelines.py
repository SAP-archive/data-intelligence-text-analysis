# -*- coding: utf-8 -*-
# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html
import pandas as pd
import re
from datetime import date
import logging
import json
#import sqlite3
#from hdfs import InsecureClient
import os

# create an SDL connection
# client_sdl = InsecureClient('http://datalake:50070')

MINIMUM_TEXT_LENGTH = 50

class OnlinemediaPipeline(object):
    '''
    all items will be thrown into the pipeline, independent from their names and the processing of items is
    on the flying, which means, once an item is generated by spider, it will be sumitted to the pipeline.
    '''
    def process_item(self, article, spider):
        '''
        url_df = pd.DataFrame(Spiegel_item['url'])
        url_df.to_csv("")
        '''
        if(bool(article['text']) is not False):
            text_test = re.sub(r'\W+', '',article['text'][0].strip())

            if(bool(text_test) is not False):
                website = article['website']
                crawl_date = str(date.today())
                rubrics = article['rubrics']
                paywall = article['paywall']
                title = ''
                if article['title']:
                    title = article['title'][0].strip()
                text = ' '.join(article['text']).strip()
                #full_article = title + '\n' + text.strip()
                article_dict = {'website': website, 'date': crawl_date, 'index': article['index'],\
                                'url': article['url'], 'rubrics': rubrics, 'paywall': paywall, 'title': title,'text': text}
                if len(text) > MINIMUM_TEXT_LENGTH  :
                    print(json.dumps(article_dict))



